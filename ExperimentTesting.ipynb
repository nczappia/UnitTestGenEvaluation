{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Experiment Structure\n",
    "* Using different models\n",
    "* Run on same sample for all models\n",
    "* Evaluate outputs after\n",
    "\n",
    "#### Pseudocode\n",
    "for every model{\n",
    "    generate 10 responses for all samples(for Acc@10)\n",
    "    store samples properly\n",
    "    \n",
    "}\n",
    "\n",
    "for each model's generated samples{\n",
    "    calculate metrics (CSR, CodeBLEU, EM, Acc@10, Edit Similarity, Line Coverage, Branch Coverage)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import multiprocessing\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "#from accelerate import PartialState\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    logging,\n",
    "    set_seed,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import SFTTrainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing DeepSeek Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:55<00:00, 13.82s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\")\n",
    "\n",
    "# config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"o_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# load model and dataset\n",
    "token = os.environ.get(\"HF_TOKEN\", None)\n",
    "# print out selected device\n",
    "#print(PartialState().process_index)\n",
    "#Check for GPU availability\n",
    "# print(torch.cuda.is_available())\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"GPU\")\n",
    "#     with torch.device(\"cuda:2\"):\n",
    "#         model = AutoModelForCausalLM.from_pretrained(\n",
    "#             \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\",\n",
    "#             quantization_config=bnb_config,\n",
    "#             attention_dropout=0.1,\n",
    "#             device_map=None, #Prevents auto device mapping - not really\n",
    "#             trust_remote_code=True,\n",
    "#         )\n",
    "#         model.to(\"cuda:2\")\n",
    "# else:\n",
    "#     #Handle no GPU availiability\n",
    "#     print(\"No GPU\")\n",
    "#     with torch.device(\"cpu\"):\n",
    "#         model = AutoModelForCausalLM.from_pretrained(\n",
    "#             \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\",\n",
    "#             quantization_config=bnb_config,\n",
    "#             attention_dropout=0.1,\n",
    "#             device_map=None, #Prevents auto device mapping - not really\n",
    "#             trust_remote_code=True,\n",
    "#         )\n",
    "#         model.to(\"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    attention_dropout=0.1,\n",
    "    device_map=None, #Prevents auto device mapping - not really\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "#if torch.cuda.device_count() > 1:\n",
    "#    print(\"Using DataParallel for multiple GPUs\")\n",
    "#    model = torch.nn.DataParallel(model)\n",
    "\n",
    "#model.to(device)\n",
    "# Freeze all except embeddings and first layer\n",
    "#for name, param in model.named_parameters():\n",
    "#    if \"model.embed_tokens\" not in name and \"model.layers.0\" not in name:\n",
    "#        param.requires_grad = False\n",
    "#    else:\n",
    "#        param.requries_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(args.dataset_name)\n",
    "#     print(args.subset)\n",
    "#     print(args.split)\n",
    "#     print(token)\n",
    "#     print(args.num_proc if args.num_proc else multiprocessing.cpu_count())\n",
    "    \n",
    "#     '''Original file from hugging face hub\n",
    "#     data = load_dataset(\n",
    "#         args.dataset_name,\n",
    "#         data_files=args.subset,\n",
    "#         split=args.split,\n",
    "#         token=token,\n",
    "#         num_proc=args.num_proc if args.num_proc else multiprocessing.cpu_count(),\n",
    "#     )'''\n",
    "\n",
    "#     data = load_dataset(\n",
    "#         'parquet', \n",
    "#         data_files=args.subset, \n",
    "#         split=args.split,\n",
    "#         num_proc=args.num_proc if args.num_proc else multiprocessing.cpu_count(),\n",
    "#     )\n",
    "\n",
    "#     print('Dataset type: ', type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breaking\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy\n",
    "\n",
    "# Define the base directories\n",
    "base_dirs = ['train', 'eval', 'test']\n",
    "\n",
    "for i in range(len(base_dirs)):\n",
    "    base_dirs[i] = '../data/methods2test_data/' + base_dirs[i]\n",
    "# List to collect all data (rows)\n",
    "data = []\n",
    "\n",
    "total_exmaples = 0\n",
    "\n",
    "is_broken = False\n",
    "base_dir = '../data/methods2test_data/train'\n",
    "# Iterate over all subdirectories (examples) in the base directory\n",
    "for example_dir in os.listdir(base_dir):\n",
    "    example_path = os.path.join(base_dir, example_dir)\n",
    "    \n",
    "    # Ensure we're dealing with directories (i.e., each example is its own directory)\n",
    "    if os.path.isdir(example_path):\n",
    "        # Use glob to get all JSON files in the example directory\n",
    "        json_files = glob(os.path.join(example_path, '*.json'))\n",
    "        \n",
    "        # Iterate over the JSON files\n",
    "        for json_file in json_files:\n",
    "            # Read the JSON data\n",
    "            with open(json_file, 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "            \n",
    "            # Add the JSON data to the data list (can modify if additional info is needed)\n",
    "            data.append(json_data)\n",
    "            total_exmaples+=1\n",
    "            if(total_exmaples>=100):\n",
    "                print(\"breaking\")\n",
    "                is_broken = True\n",
    "                break\n",
    "    if(is_broken):\n",
    "        break\n",
    "# Convert the collected data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optional: Inspect the first few rows\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_methods = df['focal_method']\n",
    "code_bodies = pd.DataFrame({'code': focal_methods.apply(lambda x: x['body'])})\n",
    "\n",
    "test_cases = df[\"test_case\"]\n",
    "test_code_bodies = pd.DataFrame({'tests': test_cases.apply(lambda x: x['body'])})\n",
    "\n",
    "code_test_df = pd.concat([code_bodies, test_code_bodies], axis=1)\n",
    "\n",
    "code_test_df['prompted_code'] = \"Here is a method implementation in Java:\\n\\n\" + code_test_df['code'] + \"\\n\\nWrite a full test class with test cases to validate the method defined above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(code_test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Experiment (Input and Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <class 'transformers_modules.deepseek-ai.DeepSeek-Coder-V2-Lite-Instruct.e434a23f91ba5b4923cf6c9d9a238eb4a08e3a11.modeling_deepseek.AddAuxiliaryLoss'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <class 'transformers_modules.deepseek-ai.DeepSeek-Coder-V2-Lite-Instruct.e434a23f91ba5b4923cf6c9d9a238eb4a08e3a11.modeling_deepseek.AddAuxiliaryLoss'>: transformers_modules.deepseek-ai.DeepSeek-Coder-V2-Lite-Instruct.e434a23f91ba5b4923cf6c9d9a238eb4a08e3a11.modeling_deepseek.AddAuxiliaryLoss has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Experiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [07:02<00:00,  4.22s/ examples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved responses to my_generated_responses.csv. Total time: 706.99 seconds\n"
     ]
    }
   ],
   "source": [
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "def generate_responses_from_df(\n",
    "    df: pd.DataFrame,\n",
    "    text_column: str,\n",
    "    text_generator,         # We pass the pipeline directly\n",
    "    max_length: int = 100,\n",
    "    output_csv: str = \"generated_responses.csv\",\n",
    "    timing_file: str = \"timing.txt\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Iterates through all rows in `df`, uses a text-generation pipeline to\n",
    "    generate responses for each row's text, and stores the results in a CSV.\n",
    "    Also logs total time to a text file.\n",
    "    \"\"\"\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    # 1) Start timing\n",
    "    print(\"Starting Experiment\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 2) Generate responses\n",
    "\n",
    "    def generate_text(batch, text_generator=None):\n",
    "        prompts = batch[\"prompted_code\"]\n",
    "        outputs = text_generator(batch[\"prompted_code\"])\n",
    "        # If the pipeline returns a list of dict, handle that\n",
    "        generated_texts = [o[0][\"generated_text\"] for o in outputs]\n",
    "        return {\"generated_text\": generated_texts}\n",
    "\n",
    "    generated_dataset = dataset.map(generate_text, \n",
    "                                    fn_kwargs= {\"text_generator\": text_generator}, \n",
    "                                    desc=\"Processing dataset\", \n",
    "                                    batched=True, \n",
    "                                    batch_size=8,\n",
    "                                    #num_proc=4,\n",
    "    )\n",
    "\n",
    "    # 3) End timing\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    # 4) Store and save results\n",
    "    df = generated_dataset.to_pandas()\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    with open(timing_file, \"w\") as f:\n",
    "        f.write(f\"Total generation time (seconds): {total_time}\\n\")\n",
    "\n",
    "    print(f\"Saved responses to {output_csv}. Total time: {total_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use the pipeline you created earlier\n",
    "    generate_responses_from_df(\n",
    "        df=code_test_df,\n",
    "        text_column=\"prompted_code\",\n",
    "        text_generator=text_generator,  # pipeline we created\n",
    "        max_length=1000,  \n",
    "        output_csv=\"my_generated_responses.csv\",\n",
    "        timing_file=\"my_timing_log.txt\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "def generate_responses_from_df(\n",
    "    df: pd.DataFrame,\n",
    "    text_column: str,\n",
    "    model_name: str,\n",
    "    device: int = -1,\n",
    "    max_length: int = 100,\n",
    "    output_csv: str = \"generated_responses.csv\",\n",
    "    timing_file: str = \"timing.txt\"\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Iterates through all rows in `df`.\n",
    "    2. Uses a text-generation pipeline to generate responses for each row's text.\n",
    "    3. Stores the responses and saves them to `output_csv`.\n",
    "    4. Logs total time taken to `timing_file`.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Initialize text-generation pipeline\n",
    "    text_generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_name,\n",
    "        device=device  # -1 = CPU; 0 or other ints = specific GPU\n",
    "    )\n",
    "\n",
    "    # 2) Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 3) Iterate through the DataFrame, generate responses\n",
    "    responses = []\n",
    "    for i, row in df.iterrows():\n",
    "        prompt_text = row[text_column]\n",
    "\n",
    "        # Generate text (You can tune these kwargsâ€”max_length, etc.)\n",
    "        output = text_generator(prompt_text, max_length=max_length, num_return_sequences=1)\n",
    "        \n",
    "        # output is a list of dicts, e.g. [{'generated_text': \"...\"}]\n",
    "        generated_text = output[0][\"generated_text\"]\n",
    "        responses.append(generated_text)\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    # 4) Add responses to DataFrame and save\n",
    "    df[\"response\"] = responses\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    # Save timing info\n",
    "    with open(timing_file, \"w\") as f:\n",
    "        f.write(f\"Total generation time (seconds): {total_time}\\n\")\n",
    "\n",
    "    print(f\"Saved responses to {output_csv}.\")\n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "\n",
    "# ------------------------\n",
    "# Example usage\n",
    "# ------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Suppose your CSV has a column \"prompt\" with text to be processed\n",
    "    df_input = pd.DataFrame({\n",
    "        \"prompt\": [\n",
    "            \"Explain why the sky is blue.\",\n",
    "            \"Write a short poem about cats.\",\n",
    "            \"Summarize the benefits of exercise.\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    generate_responses_from_df(\n",
    "        df=df_input,\n",
    "        text_column=\"prompt\",\n",
    "        model_name=\"gpt2\",      # or any other HF model, e.g. \"gpt2-medium\", \"gpt-neo-125M\", etc.\n",
    "        device=-1,              # use CPU; set to 0 for first GPU, 1 for second GPU, etc.\n",
    "        max_length=50,\n",
    "        output_csv=\"my_generated_responses.csv\",\n",
    "        timing_file=\"my_timing_log.txt\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model Training (In case of potential use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "os.makedirs(\"../models/experiment_testing\", exist_ok=True)\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# setup the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=100,\n",
    "        max_steps=10000,\n",
    "        #max_seq_length=512,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        weight_decay=0.01,\n",
    "        bf16=False,################Originally True\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        output_dir=\"../models/experiment_testing\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        seed=0,\n",
    "        run_name=f\"train-deepseekcoder-v2-methods2test\",\n",
    "        report_to=\"wandb\",\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    #dataset_text_field=\"prompted_code\",\n",
    ")\n",
    "# launch\n",
    "print(\"Training...\")\n",
    "trainer.train()\n",
    "print(\"Saving the last checkpoint of the model\")\n",
    "model.save_pretrained(os.path.join(args.output_dir, args.model_id + \"_\" + args.subset + \"_final_checkpoint/\"))\n",
    "'''if args.push_to_hub:\n",
    "    trainer.push_to_hub(\"Upload model\", token=os.getenv(\"HUGGINGFACE_TOKEN\")'''\n",
    "print(\"Training Done! ðŸ’¥\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UnitTestGeneration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
