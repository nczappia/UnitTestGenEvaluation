{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from accelerate import PartialState\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating 4-bit Quantization Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSeek-Coder-V2-Lite-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct:\n",
      "- configuration_deepseek.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct:\n",
      "- modeling_deepseek.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [45:36<00:00, 684.14s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [03:06<00:00, 46.57s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "del(model)\n",
    "del(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeLlama-13b-Instruct-hf (and 7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelLoading.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelLoading.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelLoading.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mcodellama/CodeLlama-7b-Instruct-hf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelLoading.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelLoading.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mcodellama/CodeLlama-7b-Instruct-hf\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelLoading.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     quantization_config\u001b[39m=\u001b[39;49mbnb_config, \u001b[39m#Model quantization\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelLoading.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelLoading.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     low_cpu_mem_usage\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m#Automatic with quantized models\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnick-aisec-101.cs.luc.edu/media/mujtaba/DATA/nick/UnitTestExamples/UnitTestGenEvaluation/notebooks/testing_models/ModelLoading.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/modeling_utils.py:4245\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4235\u001b[0m         load_contexts\u001b[39m.\u001b[39mappend(tp_device)\n\u001b[1;32m   4237\u001b[0m     \u001b[39mwith\u001b[39;00m ContextManagers(load_contexts):\n\u001b[1;32m   4238\u001b[0m         (\n\u001b[1;32m   4239\u001b[0m             model,\n\u001b[1;32m   4240\u001b[0m             missing_keys,\n\u001b[1;32m   4241\u001b[0m             unexpected_keys,\n\u001b[1;32m   4242\u001b[0m             mismatched_keys,\n\u001b[1;32m   4243\u001b[0m             offload_index,\n\u001b[1;32m   4244\u001b[0m             error_msgs,\n\u001b[0;32m-> 4245\u001b[0m         ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   4246\u001b[0m             model,\n\u001b[1;32m   4247\u001b[0m             state_dict,\n\u001b[1;32m   4248\u001b[0m             loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4249\u001b[0m             resolved_archive_file,\n\u001b[1;32m   4250\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   4251\u001b[0m             ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   4252\u001b[0m             sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   4253\u001b[0m             _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   4254\u001b[0m             low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   4255\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   4256\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   4257\u001b[0m             offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   4258\u001b[0m             dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   4259\u001b[0m             hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   4260\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   4261\u001b[0m             gguf_path\u001b[39m=\u001b[39;49mgguf_path,\n\u001b[1;32m   4262\u001b[0m             weights_only\u001b[39m=\u001b[39;49mweights_only,\n\u001b[1;32m   4263\u001b[0m         )\n\u001b[1;32m   4265\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4266\u001b[0m model\u001b[39m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/modeling_utils.py:4815\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4813\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4814\u001b[0m         fixed_state_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_fix_state_dict_keys_on_load(state_dict)\n\u001b[0;32m-> 4815\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   4816\u001b[0m             model_to_load,\n\u001b[1;32m   4817\u001b[0m             fixed_state_dict,\n\u001b[1;32m   4818\u001b[0m             start_prefix,\n\u001b[1;32m   4819\u001b[0m             expected_keys,\n\u001b[1;32m   4820\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   4821\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   4822\u001b[0m             offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[1;32m   4823\u001b[0m             state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[1;32m   4824\u001b[0m             state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[1;32m   4825\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   4826\u001b[0m             hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   4827\u001b[0m             is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[1;32m   4828\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   4829\u001b[0m             unexpected_keys\u001b[39m=\u001b[39;49munexpected_keys,\n\u001b[1;32m   4830\u001b[0m         )\n\u001b[1;32m   4831\u001b[0m         error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[1;32m   4832\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4833\u001b[0m     \u001b[39m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/transformers/modeling_utils.py:873\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    870\u001b[0m         param_device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m is_local_dist_rank_0() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m     \u001b[39m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mset_module_kwargs)\n\u001b[1;32m    874\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    875\u001b[0m     hf_quantizer\u001b[39m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m/media/mujtaba/DATA/nick/miniconda3/envs/UnitTestGeneration/lib/python3.9/site-packages/accelerate/utils/modeling.py:330\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    328\u001b[0m             module\u001b[39m.\u001b[39m_parameters[tensor_name] \u001b[39m=\u001b[39m param_cls(new_value, requires_grad\u001b[39m=\u001b[39mold_value\u001b[39m.\u001b[39mrequires_grad)\n\u001b[1;32m    329\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 330\u001b[0m     new_value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     new_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(value, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    quantization_config=bnb_config, #Model quantization\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True, #Automatic with quantized models\n",
    ")\n",
    "\n",
    "# del(model)\n",
    "# del(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32016, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'from typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool: \\n\"\"\" Check if in given list of numbers, are any two numbers closer to each other than given threshold. >>> has_close_elements([1.0, 2.0, 3.0], 0.5) False >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True \"\"\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = r'''Generate Unit Tests.\n",
    "\n",
    "import java.util.*;\n",
    "import java.lang.*;\n",
    "import static org.junit.jupiter.api.Assertions.*;\n",
    "import org.junit.jupiter.api.Test;\n",
    "\n",
    "\n",
    "class Solution {\n",
    "    public boolean hasCloseElements(List<Double> numbers, double threshold) {\n",
    "        for (int i = 0; i < numbers.size(); i++) {\n",
    "            for (int j = i + 1; j < numbers.size(); j++) {\n",
    "                double distance = Math.abs(numbers.get(i) - numbers.get(j));\n",
    "                if (distance < threshold) return true;\n",
    "            }\n",
    "        }\n",
    "        return false;\n",
    "    }\n",
    "}\n",
    "        \n",
    "public class SolutionTest {'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.llama.modeling_llama.LlamaForCausalLM"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Unit Tests.\n",
      "\n",
      "import java.util.*;\n",
      "import java.lang.*;\n",
      "import static org.junit.jupiter.api.Assertions.*;\n",
      "import org.junit.jupiter.api.Test;\n",
      "\n",
      "\n",
      "class Solution {\n",
      "    public boolean hasCloseElements(List<Double> numbers, double threshold) {\n",
      "        for (int i = 0; i < numbers.size(); i++) {\n",
      "            for (int j = i + 1; j < numbers.size(); j++) {\n",
      "                double distance = Math.abs(numbers.get(i) - numbers.get(j));\n",
      "                if (distance < threshold) return true;\n",
      "            }\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "        \n",
      "public class SolutionTest {\n",
      "    @Test\n",
      "    public void test1() {\n",
      "        List<Double> numbers = new ArrayList<Double>();\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(2.0);\n",
      "        numbers.add(3.0);\n",
      "        numbers.add(4.0);\n",
      "        numbers.add(5.0);\n",
      "        assertTrue(new Solution().hasCloseElements(numbers, 1.5));\n",
      "    }\n",
      "\n",
      "    @Test\n",
      "    public void test2() {\n",
      "        List<Double> numbers = new ArrayList<Double>();\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(2.0);\n",
      "        numbers.add(3.0);\n",
      "        numbers.add(4.0);\n",
      "        numbers.add(5.0);\n",
      "        assertFalse(new Solution().hasCloseElements(numbers, 1.0));\n",
      "    }\n",
      "\n",
      "    @Test\n",
      "    public void test3() {\n",
      "        List<Double> numbers = new ArrayList<Double>();\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(2.0);\n",
      "        numbers.add(3.0);\n",
      "        numbers.add(4.0);\n",
      "        numbers.add(5.0);\n",
      "        assertTrue(new Solution().hasCloseElements(numbers, 2.0));\n",
      "    }\n",
      "\n",
      "    @Test\n",
      "    public void test4() {\n",
      "        List<Double> numbers = new ArrayList<Double>();\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(2.0);\n",
      "        numbers.add(3.0);\n",
      "        numbers.add(4.0);\n",
      "        numbers.add(5.0);\n",
      "        assertTrue(new Solution().hasCloseElements(numbers, 3.0));\n",
      "    }\n",
      "\n",
      "    @Test\n",
      "    public void test5() {\n",
      "        List<Double> numbers = new ArrayList<Double>();\n",
      "        numbers.add(1.0);\n",
      "        numbers.add(2.0);\n",
      "        numbers.add(3.0);\n",
      "        numbers.add(4.0);\n",
      "        numbers.add(5.0);\n",
      "        assertTrue(new Solution().hasCloseElements(numbers, 4.0));\n",
      "    }\n",
      "\n",
      "    @Test\n",
      "    public void test6\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda:2\")\n",
    "outputs = model.generate(inputs, \n",
    "                         num_beams = 2, \n",
    "                         num_beam_groups = 2,\n",
    "                         do_sample = False, \n",
    "                         max_new_tokens = 500,\n",
    "                         top_p = 1,\n",
    "                         top_k = 50,\n",
    "                         #penalty_alpha = 0.5,\n",
    "                         length_penalty = 1.0,\n",
    "                         diversity_penalty = 0.5\n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SantaCoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/bigcode/santacoder:\n",
      "- configuration_gpt2_mq.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/bigcode/santacoder:\n",
      "- modeling_gpt2_mq.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/santacoder\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigcode/santacoder\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "del(model)\n",
    "del(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruct CodeT5P - 16b params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Salesforce/instructcodet5p-16b:\n",
      "- modeling_codet5p.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [48:11<00:00, 578.21s/it]\n",
      "CodeT5pForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:20<00:00,  4.00s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/instructcodet5p-16b\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"Salesforce/instructcodet5p-16b\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "del(model)\n",
    "del(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeT5p-770m (Alternative to CodeT5p)(https://huggingface.co/Salesforce/codet5p-770m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5p-6b\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"Salesforce/codet5p-6b\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "del(model)\n",
    "del(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeT5-Base - 770M params (Alternative to CodeT5p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"Salesforce/codet5-base\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "del(model)\n",
    "del(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeGen2.5-7b-Instruct_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen25-7b-instruct_P\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Salesforce/codegen25-7b-instruct_P\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "del(model)\n",
    "del(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeGen2 - 1B params (Alternative to codegen 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Salesforce/codegen2-1B_P:\n",
      "- configuration_codegen.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Salesforce/codegen2-1B_P:\n",
      "- modeling_codegen.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen2-1B_P\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Salesforce/codegen2-1B_P\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "del(model)\n",
    "del(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WizardCoder-15B-V1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"WizardLMTeam/WizardCoder-15B-V1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"WizardLMTeam/WizardCoder-15B-V1.0\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "del(model)\n",
    "del(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StarCoder2-3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder2-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigcode/starcoder2-3b\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "del(model)\n",
    "del(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeing Up Cache Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete models from cache\n",
    "import subprocess\n",
    "\n",
    "subprocess.run([\"rm\", \"-rf\", \"~/.cache/huggingface/hub\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete datasets from cache (Shouldn't be needed with this data)\n",
    "import subprocess\n",
    "\n",
    "subprocess.run([\"rm\", \"-rf\", \"~/.cache/huggingface/datasets\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UnitTestGeneration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
